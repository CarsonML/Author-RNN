{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load testing data and tokenizer\n",
    "with open('./testing_data3.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "with open('./tokenizer3.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n",
      "WARNING:tensorflow:Unable to restore custom metric. Please ensure that the layer implements `get_config` and `from_config` when saving. In addition, please use the `custom_objects` arg when calling `load_model()`.\n"
     ]
    }
   ],
   "source": [
    "#Load Model\n",
    "model = tf.keras.models.load_model('./model_v3_final',  compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for getting the data. IT was build for an old desing, and therefore has extra fucntionalily\n",
    "#But for getting a random starting poitn, it works fine\n",
    "def data_iteration(batch_size_per_author):\n",
    "    starts = [random.randint(0,len(data[0])-batch_size_per_author-1), random.randint(0,len(data[1])-batch_size_per_author-1), random.randint(0,len(data[2])-batch_size_per_author-1), random.randint(0,len(data[3])-batch_size_per_author-1), random.randint(0,len(data[4])-batch_size_per_author-1),random.randint(0,len(data[5])-batch_size_per_author-1)]\n",
    "    lis_total = data[0][starts[0]:starts[0]+batch_size_per_author] + data[1][starts[1]:starts[1]+batch_size_per_author] + data[2][starts[2]:starts[2]+batch_size_per_author] + data[3][starts[3]:starts[3]+batch_size_per_author] + data[4][starts[4]:starts[4]+batch_size_per_author] + data[5][starts[5]:starts[5]+batch_size_per_author]\n",
    "    X = np.asarray([x[0] for x in lis_total])\n",
    "    y_class = np.asarray([x[1] for x in lis_total])\n",
    "    y_gen = tf.keras.utils.to_categorical([x[2] for x in lis_total], num_classes =(len(tokenizer.word_index) + 1))\n",
    "    return X, y_class, y_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_per_author = 1\n",
    "totalX, total_class_y, total_gen_y = data_iteration(batch_size_per_author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70194\n"
     ]
    }
   ],
   "source": [
    "print(len(data[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#Set batch size, necessary for stateful RNN\n",
    "\n",
    "#This is the definition of all the shared layers. B/C of stateful RNN and the classifier/generator split, these will be reused in all the models\n",
    "embedding = layers.Embedding(len(tokenizer.word_index) + 1, 750, input_length=100)\n",
    "RNN1 = layers.GRU(2250, stateful=True)\n",
    "dense1 = layers.Dense(1250)\n",
    "\n",
    "#These are generator/classifier specific layers. Generator layer is shared by all diff generators. Classifier is not, but it's here for organizational consistency\n",
    "generator_dense_final = layers.Dense(len(tokenizer.word_index) + 1, activation='softmax')\n",
    "classifier_dense_final = layers.Dense(6, activation='softmax')\n",
    "#The model path. Different outputs for gen and class, but uses the same structure mostly\n",
    "inputs = keras.Input(batch_input_shape=(1,100))\n",
    "x = embedding(inputs)\n",
    "x = RNN1(x)\n",
    "# x = RNN2(x)\n",
    "# x = RNN3(x)\n",
    "x = dense1(x)\n",
    "generator_output = generator_dense_final(x)\n",
    "classifier_output = classifier_dense_final(x)\n",
    "model2 = keras.Model(\n",
    "    inputs=[inputs],\n",
    "    outputs=[generator_output, classifier_output],\n",
    ")\n",
    "keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_weights = model.get_weights()\n",
    "model2.set_weights(old_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  426     2    59    42    60 13975     5     3   174     5   297     3\n",
      "    77   224     1    70   293   481    61     1    21     3  2345    11\n",
      "    28    16 23582  4832     2  6379 10354     2    12    26    63 16716\n",
      "   163     1     4   528    50  2991  1036   466     1    23    50  5898\n",
      " 40398    81     7  2049   383     1    11  4618  5162   199     1   168\n",
      " 23583  6145     1     4  6269     1    64     3  5564     1 17389 40399\n",
      "   125     2  9085     2     3   869    11     3   265    35   216   303\n",
      "    24    29   113  1017    18    74   318    90   785   819    21   223\n",
      "    63 12423   482     2]\n",
      "(1, 100)\n",
      "[[2.9547450e-14 1.8004086e-05 6.0631868e-07 ... 2.5121595e-14\n",
      "  3.2755554e-14 2.6182685e-14]]\n",
      "55396\n",
      "1.0000014830942565\n"
     ]
    }
   ],
   "source": [
    "indiv = totalX[1]\n",
    "print(indiv)\n",
    "indiv = indiv.reshape(100,1).transpose()\n",
    "print(indiv.shape)\n",
    "result = model2.predict(indiv)\n",
    "print(result[0])\n",
    "print(result[0].shape[1])\n",
    "print(sum(result[0].tolist()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "predicted_id = random.choices(range(55396), weights = result[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n",
      "(1, 100)\n"
     ]
    }
   ],
   "source": [
    "indiv = totalX[1]\n",
    "indiv = indiv.reshape(100,1).transpose()\n",
    "len_to_predict = 100\n",
    "for i in range(len_to_predict):\n",
    "    print(indiv[:,-100:].shape)\n",
    "    result = model2.predict(indiv[:,-100:])[0].reshape(55396)\n",
    "\n",
    "    predicted_id = random.choices(range(result.shape[0]), weights = result)\n",
    "    predicted_id = np.asarray(predicted_id).reshape(1,1)\n",
    "    indiv = np.concatenate((indiv, predicted_id), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[426, 2, 59, 42, 60, 13975, 5, 3, 174, 5, 297, 3, 77, 224, 1, 70, 293, 481, 61, 1, 21, 3, 2345, 11, 28, 16, 23582, 4832, 2, 6379, 10354, 2, 12, 26, 63, 16716, 163, 1, 4, 528, 50, 2991, 1036, 466, 1, 23, 50, 5898, 40398, 81, 7, 2049, 383, 1, 11, 4618, 5162, 199, 1, 168, 23583, 6145, 1, 4, 6269, 1, 64, 3, 5564, 1, 17389, 40399, 125, 2, 9085, 2, 3, 869, 11, 3, 265, 35, 216, 303, 24, 29, 113, 1017, 18, 74, 318, 90, 785, 819, 21, 223, 63, 12423, 482, 2, 903, 1, 8, 55, 24, 123, 1, 103, 7, 76, 581, 582, 3, 1552, 262, 6, 3, 248, 2, 21, 167, 7910, 13, 86, 28, 5, 199, 4, 135, 28, 3, 90, 129, 2, 4, 58, 11, 4721, 81, 12, 60, 13, 361, 22, 4870, 12091, 2, 167, 755, 621, 42, 52, 120, 47, 20, 33, 1, 4, 8, 104, 66, 3, 114, 9, 3, 491, 2715, 2, 30, 58, 278, 159, 5, 1066, 2, 4, 39, 8, 88, 1, 720, 3300, 7, 56, 54, 108, 179, 12, 100, 19, 361, 1, 50, 6, 61, 33, 839, 9, 361, 3]\n"
     ]
    }
   ],
   "source": [
    "print(np.ndarray.tolist(indiv)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tokenizer.sequences_to_texts(np.ndarray.tolist(indiv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['please . then they will hasten to the door to call the little child , who cannot thank them , for the ice that on her lisping piled . x immortality . it is an honorable thought , and makes one lift ones hat , as one encountered gentlefolk upon a daily street , that weve immortal place , though pyramids decay , and kingdoms , like the orchard , flit russetly away . xi . the distance that the dead have gone does not at first appear ; their coming back seems possible for many an ardent year . besides , i do not think , its a good sense beyond the playing part of the world . for long hating you go on to place and look on the back way . and now that depend upon it will you turn ? rupa snarled . long months ago they were even when she said , and i dont know the house in the country club . but now im going to seek . and if i am , id tread a up or two through it before my turn , one of them said quietly in turn the']\n"
     ]
    }
   ],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
