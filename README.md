
# Author-RNN
RNN to generate prose in the styles of different authors, + analysis

# Extra Files
The processed data files and model files are too large to be put on github, so you can find them here:  
Model: https://drive.google.com/drive/folders/13wcfa4r4y05qhbyzoZhdW8Hqalv2_3Q9?usp=sharing  
Tokenizer: https://drive.google.com/file/d/1i83bPv1YwCK_2UkukiMl-ny16t2I8-w2/view?usp=sharing  
Training Data: https://drive.google.com/file/d/1RGC7KCCszHBtXn_j3DN8S0PZC3fSj8wE/view?usp=sharing   
Testing Data: https://drive.google.com/file/d/1yArRzYm8pNTv3ltyM7kyOt8BfOJHqR9C/view?usp=sharing  

# General Goal
Essentially, this project came about when I was trying to tell a friend why I liked a the style of a certain author. It's really easy to tell that different authors have different styles, but for me at least it was very hard to quantify or describe what that style was. So that got me thinking, what even IS literary style? Or to put it in the way that inspired this project in my head, what consitutes literary style? That reminded me of some other work I had done with music generation; the question then was, now that I have an AI to generate music, what actually is music rather than just noise made by an AI? So, remembering that generating neural network music had given me a much better view of the line between music and not-music, I thought I'd work on somethign similar for literary style. The goal is that generating text in the style of different authors would not only show me something about comptuers and their ability to replicate human expression (which I'm fascinated by), but also about the lines about what elements define the human expression itself, in this case the elements of literary style. Computer generation using neural networks or other models is I think a strong way to explore and study forms of expression themselves, especially in the humanities and arts, because computers can show a unique perspective of what can be replicated with just numbers and what can't (at least, can't yet :)). This project also had the added fascination bonus of being sort of an exploration of a general language AI, i.e. trying to see how computers can (eventually) understand text (and other things) beyond just being trained hyper specifically on one input style. 

# The Data
As for any project, I needed data. Since this project was focused on style differentiation, it seemed wise to choose authors with very different styles, but since I would also need to be able to evaluate the data, authors' whose style I'm familiar with. With that in mind, I chose these authors: Flannery O'Connor, William Faulkner, Shakespeare, Emily Dickinson, Charles Dickens, and Jhumpa Lahiri. The goal was to have a range of eras represented, from very old to present, as well as different types of text (I included poetry {Dickinson} and plays {Shakespeare}). The reason I chose Faulkner and O'Connor, two authors from a similar time period with similar subject matter and so somewhat violating the rule of a range of styles, was to sort of be a test on how good the style differentiation was; It essentialy creates two tasks: an easy one, differentiating between, say, Shakespeare and Dickinson, and a hard one, differentiating between Faulkner and O'Connor, and so judinging the model based on the level of task it can acheive gives a better sense of its ability. Having settled on my authors, most of their major works were copied and pasted by hand from the internet by yours truly (or, converted from pdf to text w/ python)

The data cleaning process was fairly simple: lowercase, and remove special characters. I chose to not remove punctiuation, which I think is somewhat nonstandard practice for NLP, but I think different types of punctuation are critical to literary style, which was the goal of the project. And, punctiuation was something I really wanted to analyze in terms of style, so I left it in. Then, the data was split into words, with each type of punctuation counting as a word. Then, the data was tokenized. (All of this is in process_data.py). Next, I split the data into training and testing using reduce.py. 

The last question with the data was how to feed it to the model. I created a Data class that essentialy chose n random start points (n being the batch size), then each time it was called would return the sequence of text at timestep t+1, the last time it was called it having given timestep T. This was necesary in order to be able to use a stateful model, which turned out to be critical to having decent results. It was also necessary since I had different amounts of data for each author, and I had to make sure the model got an equal amount of each.

# The Model
Essentially, my idea for the model was this: It would be an RNN with two outputs, one that classified the data by its author, and one that generated the next word. My thinking was that if the models were 90% the same and only diverged from eachother at the last dense step, then the model would have to focus on identifying the author with most of its variables, and thus the generated output would also pay attention to which author the text was from, thereby generating style. This didn't end up working perfectly as of this iteration of the project, but I have some new ideas (see last section). Apparently you can't add images to readme's as easily as you can to PR's, so I'll add an image of the graph when I figure out how. But it is essentially an embedding layer, followed by a large GRU layer, followed by a dense layer, which then splits into two, with one dense layer predicting the author and the other the next word.

There were some things I learned about what models worked best in my testing. First, the size of the embedding layer can easily be a bottleneck. I started with an embedding in 50 dimenisons, havign seem some references that worked with that size embedding. However, at that size, the loss stopped at about 6 no matter what structure I used. I think having 6 different authors creates a very large vocabulary size, hence the need for a larger embedding. Howevever, I might have gone overboard with the 750 dimension word embedding, which probaby contributed to the overfitting I saw. In the future, I'm going to test more reasonable sizes, maybe about 200. 

As to model structure with the RNN layers, I found GRU layers to be faster than LSTM layers with no discernable difference in results, so I mostly stuck with GRU's. I generally found that the embedding was the bottleneck, and the layers were fine as long as they were at least a reasonable size. However, I don't have enough data to determine whether one large layer or multiple smaller layers works best; It worked fine with one large layer, but that was with signifigant overfitting, and so its hard to tell whether any changes I made to the model RNN layers actually helped or just increased overfitting

I found that the most impactful paramater was setting "stateful" to true, i.e. using the last state at batch i as the first state for batch i+1. It signfigantly increased the training time, my guess us because it involves more complicated backpropagation. However, it was defintely worth it, as this was the step that reduced loss the most. It makes sense, since this is a generative network that works iteravely, and so therefore will always be building on itself timestep by timestep. Without the model being stateful, I found that the model often predicted the same word twice in a row, which is clearly not how the enlgish language works. However, one thing to note is that this also might have contributed to overfitting. 

The model can be found in StatefulModel.ipynb

# Results and Evaluation

The model is far from perfect. The biggest problem I currently have is overfitting. I was using google colab, and I got sort of drunk on the power of the GPU, and increased the size of everything in the model to what I realize are unreasonable levels. I thought I would be safe from overfitting since I essentiallay have one data point for each word in my dataset, which is a lot. But clearly even that can be overfitted. Ending at a loss of about 2.5 and about 70% accuracy on the training data (the loss could go even lower with more training time, but it would't be worth it because....) however, on the testing data, it had a loss of about 8 and an accuracy of about 15%. Not as great, and its clear that there's signifigant overfitting going on. That loss might be slightly incorrect, however; The way I made the testing data was to cut off the end of my dataset (so as to preserve order of data points for the stateful model), but that created a situation in which the model is likely being tested on books it never saw at all, which proabably makes the loss a bit higher than it would be otherwise. But, still, this is pretty signifigant overfitting. I'll mention some ideas for how to fix that in the next section

The model managed to generate sentences that were mostly grammatically correct, even on the testing data (I'd say about 3/4ths of the time the sentences were grammatically correct). However, they didn't make much sense, which is somewhat expected. It had a really hard time with poetry and shakespeare in particular; my guess is that for poetry, there was much less data for Emily Dickinson, meaning more overfitting. For Shakespeare, I'm not 100% sure, but I think having 4 other authors with novel data didn't help (I have some ideas for how to fix that, see next section). The model also unfortunately had some problems actually behaving differently for different authors, which, as you can recall, was kind of the whole point of the project, so there's a lot of room for improvement. 

I did some basic unigram analysis of some samples of the predictions on the test data (generated by generating sequences of 100 words off of the testing data), and found that in general, the model had the same distribution of words across all 6 authors, and differences often didn't correlate. However, there were some good results; for example, the word "reckon" was generated more off for O'Connor and Faulkner, which matched the trends in the real data. There were a couple other words like that, so I think that even if generally there were some issues with differentiation, it could defintely be improved. However, the classification had much better accuracy, but that's to be expected with a much simpler task

I had planned to do some other tests, but given the Unigram test wasn't very promising, they didn't seem like they would reveal much on this iteration of the model. I tested types of punctuation (i.e. does one author use one type of punctuation more than the other authors), but didn't find a correlation between the generated and real data. I also counted the unigrams of the samples, but didn't get much out of those tests on this iteration. I also wanted to measure sentence length, but I think on this model it wouldn't reveal much. Similar case for parts of speech tagging; I wanted to see if some authors use certain parts of speach, i.e. verbs, more than others. But I don't think my results are quite good enough for that yet. And I also plan to use bigram analysis on the data at some point, but it wouldn't reveal much at this stage in the project


The evaluating model can be found in Evaluate.ipynb

# Next Steps
As you can see from what I described about my results, there's a lot of room for improvement in the next interation of this project. I think the main issue right now is my model structure. Essentially, if the model can overfit so much, it is likely big enough to create completely separate systems for the classifications and generations, meaning it doesn't have to pay attention to style. I have some possible ideas for model structures to adress that. First would be reducing the size of some of the layers, to reduce overfitting. Also, I think I would do the classifcation step first, then concatenate the result onto each timestep in the word data. That I think might make the author a larger factor into the word generated, which is what one wants for replicating literary style. I could also try something like a categorical GAN; I've researched GAN's a lot but they don't play well with time series generation so I'd have to figure out a way around that. I also might try a bidirectional RNN, but I'd have to be careful of more overfitting. I might also experiment a bit more about using LSTMs, or maybe even get a little crazy and try a CNN or something with attention layers.

I also think generally helping overfitting would help the model. I think adding in some noise to the data would be a good idea. Also, my layers are currently ridiculously large, so reducing the sizes would likely help. If I could find a way to incorporate more data, that would help. I also have to adress the problem of some authors having much less data than others, and therefore being more affected by overfitting, but I haven't come up with a strategy for that yet, other than using different authors. I also think my attempts to introduce poetry and plays into the model might be a bit too much in this stage; I think I'll restrict it to prose until I come up with a better model.  Taking out shakespeare would also eliminate the old volcabulary words that signifigantly increase the vocabulary size.

In a sort of conclusion, I think my results showed enough promise with certain words to continue working on this project for more iterations; however, there's a lot of room for improvement, and more detailed analysis of style with likely have to wait for a slightly better model. I'll make some updates when I have a model that performs a bit better! And if anyone managed to read all that text, I'm definitely taking suggestions for other tests to run on the output!






