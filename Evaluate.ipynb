{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkVJ_-MdHxKN"
      },
      "source": [
        "# Uncomment line below, FOR COLAB ONLY: YOU NEED TO UPDATE NLTK\n",
        "#!pip install --upgrade nltk\n",
        "#reqs\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import random\n",
        "import numpy as np\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "from nltk.util import ngrams\n",
        "from nltk.lm import NgramCounter\n",
        "import matplotlib\n",
        "from operator import itemgetter\n",
        "%matplotlib inline\n",
        "\n",
        "#mount google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#load test data and tokenizer\n",
        "with open('/content/drive/MyDrive/testing_data3.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/tokenizer3.pkl', 'rb') as f:\n",
        "    tokenizer = pickle.load(f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aow-uyWSI2NN"
      },
      "source": [
        "#Some functions needed to load the model\n",
        "def get_lr_metric(optimizer):\n",
        "    def lr(y_true, y_pred):\n",
        "        return optimizer.lr\n",
        "    return lr\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "lr_metric = get_lr_metric(optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRBYtNHwJHbU"
      },
      "source": [
        "#Load Model and compile\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/saved_model3/model_v3_final', compile=False)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy',lr_metric])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVfruNKpJNpZ"
      },
      "source": [
        "#Define the data class\n",
        "#TODO: Move into its own file, so it can be used in multpile places\n",
        "class Data:\n",
        "  def __init__ (self, data, batch_size, vocab_length):\n",
        "    self.data = data\n",
        "    self.batch_size = batch_size\n",
        "    self.lengths = [len(x) for x in data]\n",
        "    self.counters = self.create_counters()\n",
        "    self.vocab_length = vocab_length\n",
        "  def next_batch(self):\n",
        "    X = []\n",
        "    y_class = []\n",
        "    y_gen = []\n",
        "    for i in range(len(self.data)):\n",
        "      for ii in range(self.batch_size):\n",
        "        X.append(data[i][self.counters[i][ii]][0])\n",
        "        y_class.append(data[i][self.counters[i][ii]][1])\n",
        "        y_gen.append(data[i][self.counters[i][ii]][2])\n",
        "    self.update_counters()\n",
        "    return np.asarray(X), keras.utils.to_categorical(y_gen, num_classes=self.vocab_length), np.asarray(y_class)\n",
        "  def create_counters(self):\n",
        "    counters = {}\n",
        "    for i in (range(len(self.data))):\n",
        "      counters[i] = random.sample(range(len(data[i])-1), self.batch_size)\n",
        "    return counters\n",
        "  def update_counters(self):\n",
        "    for key  in self.counters:\n",
        "      for i in range(len(self.counters[key])):\n",
        "        self.counters[key][i] =  (self.counters[key][i] + 1) % self.lengths[key]\n",
        "  def get_random_starts_for_testing(self, author):\n",
        "    starts = []\n",
        "    for i in range(self.batch_size*6):\n",
        "      starts.append(random.randint(0,len(data[author])-1))\n",
        "    lis_total = []\n",
        "    for i in range(self.batch_size*6):\n",
        "      lis_total.append(data[author][starts[i]])\n",
        "    X = np.asarray([x[0] for x in lis_total])\n",
        "    y_class = np.asarray([x[1] for x in lis_total])\n",
        "    y_gen = tf.keras.utils.to_categorical([x[2] for x in lis_total], num_classes =(len(tokenizer.word_index) + 1))\n",
        "    return X, y_class, y_gen\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzylZwZ_JbZv"
      },
      "source": [
        "#Define the data class\n",
        "data_getter = Data(data, 150, len(tokenizer.word_index) + 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cGRkNaNJcLm"
      },
      "source": [
        "#Test model on test data\n",
        "iter_num = 20\n",
        "for i in range(iter_num):\n",
        "    totalX, total_gen_y, total_class_y = data_getter.next_batch()\n",
        "    total_loss, loss_g, loss_c, acc_g, lr_g, acc_c, lr_c = model.test_on_batch(totalX, [total_gen_y, total_class_y], reset_metrics=False)\n",
        "print(total_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXomgZj9JfH2"
      },
      "source": [
        "#Check Accuracy\n",
        "print(acc_g)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN-R0W9uu5sN"
      },
      "source": [
        "#Create a dictionary in which to store generated samples\n",
        "samples = {}\n",
        "for author in range(6):\n",
        "    samples[author] = {}\n",
        "    samples[author]['real'] = []\n",
        "    samples[author]['fake'] = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVNExFsFKCUw"
      },
      "source": [
        "#Iterate, generate samples of length 100. Store in dictionary, separated into real and fake samples. THIS TAKES A WHILE AND COLAB LIKES TO TIME OUT :(\n",
        "num_iter = 100\n",
        "for i in range(num_iter):\n",
        "  print(i)\n",
        "  for author in range(6):\n",
        "    test = data_getter.get_random_starts_for_testing(author)\n",
        "    X = test[0]\n",
        "    for ii in range(100):\n",
        "      results = model.predict_on_batch(X[:,-100:])\n",
        "      processed_results = np.argmax(results[0], axis=1).reshape(900,1)\n",
        "      X = np.concatenate((X, processed_results ), axis =1)\n",
        "    samples[author]['real'].append(np.asarray(tokenizer.sequences_to_texts(X[:,:100].tolist())))\n",
        "    samples[author]['fake'].append(np.asarray(tokenizer.sequences_to_texts(X[:,-100:].tolist())))\n",
        "    print(\"completed\")\n",
        "  with open('/content/drive/MyDrive/evaluate_data', 'wb+') as file:\n",
        "      pickle.dump(samples, file)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzCzee0IiLOp"
      },
      "source": [
        "#Load samples, if they've already been generated\n",
        "with open('/content/drive/MyDrive/evaluate_data', 'rb') as file:\n",
        "    samples = pickle.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NVbWZ6BQVtr"
      },
      "source": [
        "#Concatenate data so it's easier to work with\n",
        "for key in samples:\n",
        "  samples[key]['real'] = np.concatenate(samples[key]['real'], axis=0)\n",
        "  samples[key]['fake'] = np.concatenate(samples[key]['fake'], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlxaAOZjgcDh"
      },
      "source": [
        "#Create Ngram counter for Unigrams, Bigrams, and Trigrams. Then save it\n",
        "ngram_total = {}\n",
        "for key in samples:\n",
        "  ngram_total[key] = {}\n",
        "  for truth in ['real', 'fake']:\n",
        "    print(\"starting\")\n",
        "    print(samples[key][truth].tolist()[0].split())\n",
        "    text_trigrams = [ngrams(sent.split(), 3) for sent in samples[key][truth].tolist()]\n",
        "    text_bigrams = [ngrams(sent.split(), 2) for sent in samples[key][truth]]\n",
        "    text_unigrams = [ngrams(sent.split(), 1) for sent in samples[key][truth]]\n",
        "    ngram_counts = NgramCounter(text_bigrams + text_unigrams + text_trigrams)\n",
        "    ngram_total[key][truth] = {}\n",
        "    ngram_total[key][truth]['ngrams'] = [text_unigrams, text_bigrams, text_trigrams]\n",
        "    ngram_total[key][truth]['counter'] = ngram_counts\n",
        "    print(\"ending\")\n",
        "with open('/content/drive/MyDrive/ngrams_data', 'wb+') as file:\n",
        "    pickle.dump(ngram_total, file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rHC4JWThVcs"
      },
      "source": [
        "# This checks a certain word, to look for things like a word being especially prevalent in one text, etc\n",
        "\n",
        "# freq_dict = {}\n",
        "for author in range(6):\n",
        "  # freq_dict[author] = {}\n",
        "  for truth in ['real','fake']:\n",
        "    print(ngram_total[author][truth]['counter'][1]['expect'])\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9qGy6_zGAXw"
      },
      "source": [
        "#This function prints all the unigrams, as well as their collections in the text. Used to looks for words taht are present more in one text than another\n",
        "# freq_dict = {}\n",
        "\n",
        "# freq_dict[author] = {}\n",
        "for truth in ['fake']:\n",
        "  for author in range(6):\n",
        "    for item in ngram_total[author][truth]['counter'][1]:\n",
        "      total = [item]\n",
        "      for author2 in range(6):\n",
        "        total.append(ngram_total[author2][truth]['counter'][1][item])\n",
        "      print(total)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nlLNz_cMMEm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}